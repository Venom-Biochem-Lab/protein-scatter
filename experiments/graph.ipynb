{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./wandb_export_2024-03-09T17_43_52.614-08_00.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpt1709417396.5932615 - val_loss</th>\n",
       "      <th>gpt1709446151.2975984 - val_loss</th>\n",
       "      <th>gpt1709413573.4745002 - val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.537348</td>\n",
       "      <td>1.428721</td>\n",
       "      <td>3.174634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.509863</td>\n",
       "      <td>1.444985</td>\n",
       "      <td>2.343646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.531592</td>\n",
       "      <td>1.412735</td>\n",
       "      <td>2.264296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.519633</td>\n",
       "      <td>1.434165</td>\n",
       "      <td>2.157284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.521160</td>\n",
       "      <td>1.423881</td>\n",
       "      <td>2.109746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.388365</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.379090</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.395109</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.364590</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.393410</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>201 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     gpt1709417396.5932615 - val_loss  gpt1709446151.2975984 - val_loss  \\\n",
       "0                            1.537348                          1.428721   \n",
       "1                            1.509863                          1.444985   \n",
       "2                            1.531592                          1.412735   \n",
       "3                            1.519633                          1.434165   \n",
       "4                            1.521160                          1.423881   \n",
       "..                                ...                               ...   \n",
       "196                               NaN                          1.388365   \n",
       "197                               NaN                          1.379090   \n",
       "198                               NaN                          1.395109   \n",
       "199                               NaN                          1.364590   \n",
       "200                               NaN                          1.393410   \n",
       "\n",
       "     gpt1709413573.4745002 - val_loss  \n",
       "0                            3.174634  \n",
       "1                            2.343646  \n",
       "2                            2.264296  \n",
       "3                            2.157284  \n",
       "4                            2.109746  \n",
       "..                                ...  \n",
       "196                               NaN  \n",
       "197                               NaN  \n",
       "198                               NaN  \n",
       "199                               NaN  \n",
       "200                               NaN  \n",
       "\n",
       "[201 rows x 3 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"gpt1709417396.5932615\"\n",
    "b = \"gpt1709446151.2975984\"\n",
    "c = \"gpt1709413573.4745002\"\n",
    "val_loss = lambda x: f\"{x} - val_loss\"\n",
    "train_loss = lambda x: f\"{x} - train_loss\"\n",
    "df[[val_loss(a), val_loss(b), val_loss(c)]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dft = pd.read_csv(\"./wandb_export_2024-03-09T17_48_14.119-08_00.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa = pd.DataFrame(\n",
    "    {\n",
    "        \"train_loss\": df[[val_loss(a)]].dropna()[val_loss(a)].tolist(),\n",
    "        \"val_loss\": dft[[train_loss(a)]].dropna()[train_loss(a)].tolist(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.537348</td>\n",
       "      <td>1.469540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.509863</td>\n",
       "      <td>1.467791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.531592</td>\n",
       "      <td>1.450578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.519633</td>\n",
       "      <td>1.443451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.521160</td>\n",
       "      <td>1.461614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1.433586</td>\n",
       "      <td>1.289069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1.424680</td>\n",
       "      <td>1.296585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1.428810</td>\n",
       "      <td>1.296323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1.429815</td>\n",
       "      <td>1.309269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>1.440095</td>\n",
       "      <td>1.304240</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     train_loss  val_loss\n",
       "0      1.537348  1.469540\n",
       "1      1.509863  1.467791\n",
       "2      1.531592  1.450578\n",
       "3      1.519633  1.443451\n",
       "4      1.521160  1.461614\n",
       "..          ...       ...\n",
       "96     1.433586  1.289069\n",
       "97     1.424680  1.296585\n",
       "98     1.428810  1.296323\n",
       "99     1.429815  1.309269\n",
       "100    1.440095  1.304240\n",
       "\n",
       "[101 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfb = pd.DataFrame(\n",
    "    {\n",
    "        \"train_loss\": df[[val_loss(b)]].dropna()[val_loss(b)].tolist(),\n",
    "        \"val_loss\": dft[[train_loss(b)]].dropna()[train_loss(b)].tolist(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = pd.DataFrame(\n",
    "    {\n",
    "        \"train_loss\": df[[val_loss(c)]].dropna()[val_loss(c)].tolist(),\n",
    "        \"val_loss\": dft[[train_loss(c)]].dropna()[train_loss(c)].tolist(),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pd.concat((dfc, dfa, dfb ))\n",
    "d[\"iteration\"] = [i*100 for i in range(0, len(d))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/altair/utils/core.py:317: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for col_name, dtype in df.dtypes.iteritems():\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-9f863caf3a624e50a125d1778b19163d\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-9f863caf3a624e50a125d1778b19163d\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-9f863caf3a624e50a125d1778b19163d\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"layer\": [{\"mark\": \"line\", \"encoding\": {\"x\": {\"field\": \"iteration\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"val_loss\", \"type\": \"quantitative\"}}}, {\"mark\": \"line\", \"encoding\": {\"x\": {\"field\": \"iteration\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"train_loss\", \"type\": \"quantitative\"}}}], \"data\": {\"name\": \"data-9367de3b7501009a0738af6fe4c38dad\"}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.17.0.json\", \"datasets\": {\"data-9367de3b7501009a0738af6fe4c38dad\": [{\"train_loss\": 3.174633502960205, \"val_loss\": 3.1711435317993164, \"iteration\": 0}, {\"train_loss\": 2.343646287918091, \"val_loss\": 2.3368287086486816, \"iteration\": 100}, {\"train_loss\": 2.264296054840088, \"val_loss\": 2.260842800140381, \"iteration\": 200}, {\"train_loss\": 2.1572835445404053, \"val_loss\": 2.1579086780548096, \"iteration\": 300}, {\"train_loss\": 2.109746217727661, \"val_loss\": 2.1093811988830566, \"iteration\": 400}, {\"train_loss\": 2.066469430923462, \"val_loss\": 2.071231842041016, \"iteration\": 500}, {\"train_loss\": 2.0468196868896484, \"val_loss\": 2.030409336090088, \"iteration\": 600}, {\"train_loss\": 2.0181667804718018, \"val_loss\": 2.019953966140747, \"iteration\": 700}, {\"train_loss\": 2.013627767562866, \"val_loss\": 2.0179367065429688, \"iteration\": 800}, {\"train_loss\": 2.000584602355957, \"val_loss\": 1.9980149269104004, \"iteration\": 900}, {\"train_loss\": 1.9754421710968015, \"val_loss\": 1.9771215915679927, \"iteration\": 1000}, {\"train_loss\": 1.9748300313949585, \"val_loss\": 1.9714171886444087, \"iteration\": 1100}, {\"train_loss\": 1.9571478366851809, \"val_loss\": 1.960882306098938, \"iteration\": 1200}, {\"train_loss\": 1.947417140007019, \"val_loss\": 1.9476193189620967, \"iteration\": 1300}, {\"train_loss\": 1.9323012828826904, \"val_loss\": 1.925997257232666, \"iteration\": 1400}, {\"train_loss\": 1.918534755706787, \"val_loss\": 1.9315930604934688, \"iteration\": 1500}, {\"train_loss\": 1.911191821098328, \"val_loss\": 1.898911952972412, \"iteration\": 1600}, {\"train_loss\": 1.8963879346847532, \"val_loss\": 1.8833661079406736, \"iteration\": 1700}, {\"train_loss\": 1.8847426176071167, \"val_loss\": 1.8869363069534304, \"iteration\": 1800}, {\"train_loss\": 1.887333989143372, \"val_loss\": 1.8738707304000852, \"iteration\": 1900}, {\"train_loss\": 1.8615493774414065, \"val_loss\": 1.8614137172698972, \"iteration\": 2000}, {\"train_loss\": 1.845096468925476, \"val_loss\": 1.8525463342666624, \"iteration\": 2100}, {\"train_loss\": 1.8498557806015012, \"val_loss\": 1.858132004737854, \"iteration\": 2200}, {\"train_loss\": 1.842276930809021, \"val_loss\": 1.8313915729522705, \"iteration\": 2300}, {\"train_loss\": 1.8241772651672363, \"val_loss\": 1.8404967784881592, \"iteration\": 2400}, {\"train_loss\": 1.8296890258789065, \"val_loss\": 1.8321681022644043, \"iteration\": 2500}, {\"train_loss\": 1.8099687099456787, \"val_loss\": 1.8034722805023196, \"iteration\": 2600}, {\"train_loss\": 1.8064132928848269, \"val_loss\": 1.7950031757354736, \"iteration\": 2700}, {\"train_loss\": 1.8078001737594604, \"val_loss\": 1.7891427278518677, \"iteration\": 2800}, {\"train_loss\": 1.795107364654541, \"val_loss\": 1.7638750076293943, \"iteration\": 2900}, {\"train_loss\": 1.7883785963058472, \"val_loss\": 1.7707196474075315, \"iteration\": 3000}, {\"train_loss\": 1.7762293815612793, \"val_loss\": 1.763066291809082, \"iteration\": 3100}, {\"train_loss\": 1.7552549839019775, \"val_loss\": 1.7682876586914062, \"iteration\": 3200}, {\"train_loss\": 1.7604494094848633, \"val_loss\": 1.7415460348129272, \"iteration\": 3300}, {\"train_loss\": 1.7496181726455688, \"val_loss\": 1.749006986618042, \"iteration\": 3400}, {\"train_loss\": 1.758487343788147, \"val_loss\": 1.7407416105270386, \"iteration\": 3500}, {\"train_loss\": 1.7434532642364502, \"val_loss\": 1.7361804246902466, \"iteration\": 3600}, {\"train_loss\": 1.730815887451172, \"val_loss\": 1.72517991065979, \"iteration\": 3700}, {\"train_loss\": 1.73474383354187, \"val_loss\": 1.719598412513733, \"iteration\": 3800}, {\"train_loss\": 1.740984320640564, \"val_loss\": 1.7245569229125977, \"iteration\": 3900}, {\"train_loss\": 1.7281785011291504, \"val_loss\": 1.69881272315979, \"iteration\": 4000}, {\"train_loss\": 1.7107129096984863, \"val_loss\": 1.705052375793457, \"iteration\": 4100}, {\"train_loss\": 1.698079228401184, \"val_loss\": 1.683743238449097, \"iteration\": 4200}, {\"train_loss\": 1.7006769180297852, \"val_loss\": 1.685150146484375, \"iteration\": 4300}, {\"train_loss\": 1.6919410228729248, \"val_loss\": 1.674114227294922, \"iteration\": 4400}, {\"train_loss\": 1.6869806051254272, \"val_loss\": 1.6792608499526978, \"iteration\": 4500}, {\"train_loss\": 1.695953369140625, \"val_loss\": 1.6630675792694092, \"iteration\": 4600}, {\"train_loss\": 1.691136121749878, \"val_loss\": 1.6751521825790403, \"iteration\": 4700}, {\"train_loss\": 1.650321364402771, \"val_loss\": 1.6616489887237549, \"iteration\": 4800}, {\"train_loss\": 1.663013935089111, \"val_loss\": 1.651251196861267, \"iteration\": 4900}, {\"train_loss\": 1.67754328250885, \"val_loss\": 1.646867036819458, \"iteration\": 5000}, {\"train_loss\": 1.6669578552246094, \"val_loss\": 1.6410211324691772, \"iteration\": 5100}, {\"train_loss\": 1.6757571697235107, \"val_loss\": 1.6388657093048096, \"iteration\": 5200}, {\"train_loss\": 1.6513736248016355, \"val_loss\": 1.6268553733825684, \"iteration\": 5300}, {\"train_loss\": 1.6487051248550415, \"val_loss\": 1.616611123085022, \"iteration\": 5400}, {\"train_loss\": 1.639439582824707, \"val_loss\": 1.6221356391906738, \"iteration\": 5500}, {\"train_loss\": 1.6435492038726809, \"val_loss\": 1.62993586063385, \"iteration\": 5600}, {\"train_loss\": 1.6430743932724, \"val_loss\": 1.625796675682068, \"iteration\": 5700}, {\"train_loss\": 1.6340283155441284, \"val_loss\": 1.608188271522522, \"iteration\": 5800}, {\"train_loss\": 1.6395928859710691, \"val_loss\": 1.605008244514465, \"iteration\": 5900}, {\"train_loss\": 1.606376051902771, \"val_loss\": 1.6059669256210327, \"iteration\": 6000}, {\"train_loss\": 1.618971824645996, \"val_loss\": 1.5955450534820557, \"iteration\": 6100}, {\"train_loss\": 1.6202969551086426, \"val_loss\": 1.5905990600585938, \"iteration\": 6200}, {\"train_loss\": 1.6154921054840088, \"val_loss\": 1.5841823816299438, \"iteration\": 6300}, {\"train_loss\": 1.61579167842865, \"val_loss\": 1.5768589973449707, \"iteration\": 6400}, {\"train_loss\": 1.6241614818572998, \"val_loss\": 1.5769037008285522, \"iteration\": 6500}, {\"train_loss\": 1.6054296493530271, \"val_loss\": 1.57123601436615, \"iteration\": 6600}, {\"train_loss\": 1.6091290712356567, \"val_loss\": 1.5624014139175415, \"iteration\": 6700}, {\"train_loss\": 1.593799710273743, \"val_loss\": 1.5726453065872192, \"iteration\": 6800}, {\"train_loss\": 1.6098010540008545, \"val_loss\": 1.563808560371399, \"iteration\": 6900}, {\"train_loss\": 1.6096572875976562, \"val_loss\": 1.5600072145462036, \"iteration\": 7000}, {\"train_loss\": 1.5894689559936523, \"val_loss\": 1.5587555170059204, \"iteration\": 7100}, {\"train_loss\": 1.587715744972229, \"val_loss\": 1.5310158729553225, \"iteration\": 7200}, {\"train_loss\": 1.5895837545394895, \"val_loss\": 1.5377551317214966, \"iteration\": 7300}, {\"train_loss\": 1.5888049602508545, \"val_loss\": 1.5316994190216064, \"iteration\": 7400}, {\"train_loss\": 1.5862817764282229, \"val_loss\": 1.5343029499053955, \"iteration\": 7500}, {\"train_loss\": 1.5862940549850464, \"val_loss\": 1.5354994535446167, \"iteration\": 7600}, {\"train_loss\": 1.5742652416229248, \"val_loss\": 1.5283119678497314, \"iteration\": 7700}, {\"train_loss\": 1.581096649169922, \"val_loss\": 1.5291208028793335, \"iteration\": 7800}, {\"train_loss\": 1.5776406526565552, \"val_loss\": 1.530235767364502, \"iteration\": 7900}, {\"train_loss\": 1.5839457511901855, \"val_loss\": 1.517377495765686, \"iteration\": 8000}, {\"train_loss\": 1.5707019567489624, \"val_loss\": 1.5218333005905151, \"iteration\": 8100}, {\"train_loss\": 1.5685794353485107, \"val_loss\": 1.5108423233032229, \"iteration\": 8200}, {\"train_loss\": 1.5782437324523926, \"val_loss\": 1.4977951049804688, \"iteration\": 8300}, {\"train_loss\": 1.5710699558258057, \"val_loss\": 1.511924386024475, \"iteration\": 8400}, {\"train_loss\": 1.5609480142593384, \"val_loss\": 1.479661226272583, \"iteration\": 8500}, {\"train_loss\": 1.5589758157730105, \"val_loss\": 1.510469913482666, \"iteration\": 8600}, {\"train_loss\": 1.5470162630081177, \"val_loss\": 1.5047638416290283, \"iteration\": 8700}, {\"train_loss\": 1.5712906122207642, \"val_loss\": 1.518789529800415, \"iteration\": 8800}, {\"train_loss\": 1.5497708320617676, \"val_loss\": 1.4881842136383057, \"iteration\": 8900}, {\"train_loss\": 1.550309181213379, \"val_loss\": 1.4806632995605469, \"iteration\": 9000}, {\"train_loss\": 1.554415941238403, \"val_loss\": 1.4909199476242063, \"iteration\": 9100}, {\"train_loss\": 1.5468298196792605, \"val_loss\": 1.490040302276611, \"iteration\": 9200}, {\"train_loss\": 1.5443713665008545, \"val_loss\": 1.4850566387176514, \"iteration\": 9300}, {\"train_loss\": 1.5387338399887085, \"val_loss\": 1.4736248254776, \"iteration\": 9400}, {\"train_loss\": 1.5351758003234863, \"val_loss\": 1.4694299697875977, \"iteration\": 9500}, {\"train_loss\": 1.509495496749878, \"val_loss\": 1.4879909753799438, \"iteration\": 9600}, {\"train_loss\": 1.5408222675323486, \"val_loss\": 1.462023138999939, \"iteration\": 9700}, {\"train_loss\": 1.5220662355422974, \"val_loss\": 1.4764660596847534, \"iteration\": 9800}, {\"train_loss\": 1.5372529029846191, \"val_loss\": 1.4564094543457031, \"iteration\": 9900}, {\"train_loss\": 1.5259486436843872, \"val_loss\": 1.4604121446609497, \"iteration\": 10000}, {\"train_loss\": 1.5373482704162598, \"val_loss\": 1.469540238380432, \"iteration\": 10100}, {\"train_loss\": 1.5098631381988523, \"val_loss\": 1.4677910804748535, \"iteration\": 10200}, {\"train_loss\": 1.5315916538238523, \"val_loss\": 1.4505778551101685, \"iteration\": 10300}, {\"train_loss\": 1.5196325778961182, \"val_loss\": 1.443450927734375, \"iteration\": 10400}, {\"train_loss\": 1.521159529685974, \"val_loss\": 1.4616143703460691, \"iteration\": 10500}, {\"train_loss\": 1.5373945236206057, \"val_loss\": 1.4525202512741089, \"iteration\": 10600}, {\"train_loss\": 1.522964358329773, \"val_loss\": 1.4578441381454468, \"iteration\": 10700}, {\"train_loss\": 1.5065289735794067, \"val_loss\": 1.4548035860061646, \"iteration\": 10800}, {\"train_loss\": 1.5341116189956665, \"val_loss\": 1.4497445821762085, \"iteration\": 10900}, {\"train_loss\": 1.515536189079285, \"val_loss\": 1.4479869604110718, \"iteration\": 11000}, {\"train_loss\": 1.5140626430511477, \"val_loss\": 1.439038872718811, \"iteration\": 11100}, {\"train_loss\": 1.5073400735855105, \"val_loss\": 1.4243191480636597, \"iteration\": 11200}, {\"train_loss\": 1.5185388326644895, \"val_loss\": 1.4295952320098877, \"iteration\": 11300}, {\"train_loss\": 1.5119154453277588, \"val_loss\": 1.431149125099182, \"iteration\": 11400}, {\"train_loss\": 1.504073143005371, \"val_loss\": 1.4312068223953247, \"iteration\": 11500}, {\"train_loss\": 1.493838906288147, \"val_loss\": 1.4299715757369995, \"iteration\": 11600}, {\"train_loss\": 1.5060008764266968, \"val_loss\": 1.421009182929993, \"iteration\": 11700}, {\"train_loss\": 1.4908524751663208, \"val_loss\": 1.4178833961486816, \"iteration\": 11800}, {\"train_loss\": 1.5007740259170532, \"val_loss\": 1.4186792373657229, \"iteration\": 11900}, {\"train_loss\": 1.5096189975738523, \"val_loss\": 1.4311062097549438, \"iteration\": 12000}, {\"train_loss\": 1.498410940170288, \"val_loss\": 1.3972896337509155, \"iteration\": 12100}, {\"train_loss\": 1.4893348217010498, \"val_loss\": 1.4178061485290527, \"iteration\": 12200}, {\"train_loss\": 1.490923285484314, \"val_loss\": 1.4064675569534302, \"iteration\": 12300}, {\"train_loss\": 1.4969124794006348, \"val_loss\": 1.4222207069396973, \"iteration\": 12400}, {\"train_loss\": 1.487562894821167, \"val_loss\": 1.3941504955291748, \"iteration\": 12500}, {\"train_loss\": 1.4812216758728027, \"val_loss\": 1.4061616659164429, \"iteration\": 12600}, {\"train_loss\": 1.4874675273895264, \"val_loss\": 1.4116519689559937, \"iteration\": 12700}, {\"train_loss\": 1.4939428567886353, \"val_loss\": 1.4132201671600342, \"iteration\": 12800}, {\"train_loss\": 1.4923720359802246, \"val_loss\": 1.4002058506011963, \"iteration\": 12900}, {\"train_loss\": 1.487898588180542, \"val_loss\": 1.4082798957824707, \"iteration\": 13000}, {\"train_loss\": 1.4855782985687256, \"val_loss\": 1.3959908485412598, \"iteration\": 13100}, {\"train_loss\": 1.486487627029419, \"val_loss\": 1.4004818201065063, \"iteration\": 13200}, {\"train_loss\": 1.4922350645065308, \"val_loss\": 1.396450757980347, \"iteration\": 13300}, {\"train_loss\": 1.4907937049865725, \"val_loss\": 1.380910038948059, \"iteration\": 13400}, {\"train_loss\": 1.4795336723327637, \"val_loss\": 1.378198266029358, \"iteration\": 13500}, {\"train_loss\": 1.4857122898101809, \"val_loss\": 1.3950130939483645, \"iteration\": 13600}, {\"train_loss\": 1.4802157878875732, \"val_loss\": 1.4034826755523682, \"iteration\": 13700}, {\"train_loss\": 1.4867817163467407, \"val_loss\": 1.387570858001709, \"iteration\": 13800}, {\"train_loss\": 1.4665374755859375, \"val_loss\": 1.3880535364151, \"iteration\": 13900}, {\"train_loss\": 1.4814621210098269, \"val_loss\": 1.3754409551620483, \"iteration\": 14000}, {\"train_loss\": 1.4762623310089111, \"val_loss\": 1.3902792930603027, \"iteration\": 14100}, {\"train_loss\": 1.474129319190979, \"val_loss\": 1.362725019454956, \"iteration\": 14200}, {\"train_loss\": 1.4745469093322754, \"val_loss\": 1.372874140739441, \"iteration\": 14300}, {\"train_loss\": 1.476269245147705, \"val_loss\": 1.3861379623413086, \"iteration\": 14400}, {\"train_loss\": 1.4738496541976929, \"val_loss\": 1.374376654624939, \"iteration\": 14500}, {\"train_loss\": 1.4712371826171875, \"val_loss\": 1.3670578002929688, \"iteration\": 14600}, {\"train_loss\": 1.4557881355285645, \"val_loss\": 1.3838311433792114, \"iteration\": 14700}, {\"train_loss\": 1.4499917030334473, \"val_loss\": 1.376012682914734, \"iteration\": 14800}, {\"train_loss\": 1.4666223526000977, \"val_loss\": 1.3597135543823242, \"iteration\": 14900}, {\"train_loss\": 1.4617869853973389, \"val_loss\": 1.372511863708496, \"iteration\": 15000}, {\"train_loss\": 1.4694339036941528, \"val_loss\": 1.3658467531204224, \"iteration\": 15100}, {\"train_loss\": 1.459128737449646, \"val_loss\": 1.3545660972595217, \"iteration\": 15200}, {\"train_loss\": 1.4512029886245728, \"val_loss\": 1.357603907585144, \"iteration\": 15300}, {\"train_loss\": 1.4509600400924685, \"val_loss\": 1.355486273765564, \"iteration\": 15400}, {\"train_loss\": 1.4679956436157229, \"val_loss\": 1.3607977628707886, \"iteration\": 15500}, {\"train_loss\": 1.4596290588378906, \"val_loss\": 1.3660826683044434, \"iteration\": 15600}, {\"train_loss\": 1.4609951972961426, \"val_loss\": 1.3673254251480105, \"iteration\": 15700}, {\"train_loss\": 1.4576170444488523, \"val_loss\": 1.3683847188949585, \"iteration\": 15800}, {\"train_loss\": 1.4663482904434204, \"val_loss\": 1.3647103309631348, \"iteration\": 15900}, {\"train_loss\": 1.4565865993499756, \"val_loss\": 1.3435685634613037, \"iteration\": 16000}, {\"train_loss\": 1.460514783859253, \"val_loss\": 1.356916904449463, \"iteration\": 16100}, {\"train_loss\": 1.4550739526748655, \"val_loss\": 1.3386344909667969, \"iteration\": 16200}, {\"train_loss\": 1.4523555040359497, \"val_loss\": 1.344956874847412, \"iteration\": 16300}, {\"train_loss\": 1.4462662935256958, \"val_loss\": 1.3522131443023682, \"iteration\": 16400}, {\"train_loss\": 1.462696194648743, \"val_loss\": 1.3481149673461914, \"iteration\": 16500}, {\"train_loss\": 1.4488768577575684, \"val_loss\": 1.3447376489639282, \"iteration\": 16600}, {\"train_loss\": 1.4461439847946167, \"val_loss\": 1.346281886100769, \"iteration\": 16700}, {\"train_loss\": 1.4517853260040283, \"val_loss\": 1.3164432048797607, \"iteration\": 16800}, {\"train_loss\": 1.4569265842437744, \"val_loss\": 1.3480054140090942, \"iteration\": 16900}, {\"train_loss\": 1.4373544454574585, \"val_loss\": 1.3168232440948486, \"iteration\": 17000}, {\"train_loss\": 1.4435397386550903, \"val_loss\": 1.337807059288025, \"iteration\": 17100}, {\"train_loss\": 1.451746702194214, \"val_loss\": 1.342207670211792, \"iteration\": 17200}, {\"train_loss\": 1.4435980319976809, \"val_loss\": 1.3266202211380005, \"iteration\": 17300}, {\"train_loss\": 1.4376044273376465, \"val_loss\": 1.3384575843811035, \"iteration\": 17400}, {\"train_loss\": 1.4552905559539795, \"val_loss\": 1.3353580236434937, \"iteration\": 17500}, {\"train_loss\": 1.4287164211273191, \"val_loss\": 1.3201241493225098, \"iteration\": 17600}, {\"train_loss\": 1.4371179342269895, \"val_loss\": 1.3319398164749146, \"iteration\": 17700}, {\"train_loss\": 1.4531636238098145, \"val_loss\": 1.314613699913025, \"iteration\": 17800}, {\"train_loss\": 1.4478049278259275, \"val_loss\": 1.3164777755737305, \"iteration\": 17900}, {\"train_loss\": 1.4677422046661377, \"val_loss\": 1.3262439966201782, \"iteration\": 18000}, {\"train_loss\": 1.4410500526428225, \"val_loss\": 1.3261406421661377, \"iteration\": 18100}, {\"train_loss\": 1.4472156763076782, \"val_loss\": 1.3131203651428225, \"iteration\": 18200}, {\"train_loss\": 1.443945288658142, \"val_loss\": 1.31907856464386, \"iteration\": 18300}, {\"train_loss\": 1.446108341217041, \"val_loss\": 1.3210420608520508, \"iteration\": 18400}, {\"train_loss\": 1.4441142082214355, \"val_loss\": 1.3147791624069214, \"iteration\": 18500}, {\"train_loss\": 1.431396484375, \"val_loss\": 1.327871799468994, \"iteration\": 18600}, {\"train_loss\": 1.4295189380645752, \"val_loss\": 1.3164249658584597, \"iteration\": 18700}, {\"train_loss\": 1.4365530014038086, \"val_loss\": 1.319468975067139, \"iteration\": 18800}, {\"train_loss\": 1.4320473670959473, \"val_loss\": 1.3020869493484497, \"iteration\": 18900}, {\"train_loss\": 1.430056095123291, \"val_loss\": 1.309780836105347, \"iteration\": 19000}, {\"train_loss\": 1.4322075843811035, \"val_loss\": 1.305783748626709, \"iteration\": 19100}, {\"train_loss\": 1.4473682641983032, \"val_loss\": 1.3037714958190918, \"iteration\": 19200}, {\"train_loss\": 1.444018840789795, \"val_loss\": 1.306807518005371, \"iteration\": 19300}, {\"train_loss\": 1.4390313625335691, \"val_loss\": 1.299938678741455, \"iteration\": 19400}, {\"train_loss\": 1.4289140701293943, \"val_loss\": 1.3127317428588867, \"iteration\": 19500}, {\"train_loss\": 1.4242101907730105, \"val_loss\": 1.3088113069534302, \"iteration\": 19600}, {\"train_loss\": 1.4335857629776, \"val_loss\": 1.289068579673767, \"iteration\": 19700}, {\"train_loss\": 1.424680233001709, \"val_loss\": 1.2965854406356812, \"iteration\": 19800}, {\"train_loss\": 1.4288095235824585, \"val_loss\": 1.29632306098938, \"iteration\": 19900}, {\"train_loss\": 1.4298148155212402, \"val_loss\": 1.309268593788147, \"iteration\": 20000}, {\"train_loss\": 1.4400949478149414, \"val_loss\": 1.304240107536316, \"iteration\": 20100}, {\"train_loss\": 1.4287211894989014, \"val_loss\": 1.2920317649841309, \"iteration\": 20200}, {\"train_loss\": 1.444984793663025, \"val_loss\": 1.3139894008636477, \"iteration\": 20300}, {\"train_loss\": 1.412735104560852, \"val_loss\": 1.3005938529968262, \"iteration\": 20400}, {\"train_loss\": 1.4341648817062378, \"val_loss\": 1.283216118812561, \"iteration\": 20500}, {\"train_loss\": 1.4238805770874023, \"val_loss\": 1.3014206886291504, \"iteration\": 20600}, {\"train_loss\": 1.4172402620315552, \"val_loss\": 1.280501127243042, \"iteration\": 20700}, {\"train_loss\": 1.4440613985061646, \"val_loss\": 1.2809672355651855, \"iteration\": 20800}, {\"train_loss\": 1.4073010683059692, \"val_loss\": 1.2792495489120483, \"iteration\": 20900}, {\"train_loss\": 1.437896728515625, \"val_loss\": 1.2981585264205933, \"iteration\": 21000}, {\"train_loss\": 1.4317210912704468, \"val_loss\": 1.2987972497940063, \"iteration\": 21100}, {\"train_loss\": 1.4375410079956057, \"val_loss\": 1.2853244543075562, \"iteration\": 21200}, {\"train_loss\": 1.431942582130432, \"val_loss\": 1.2727980613708496, \"iteration\": 21300}, {\"train_loss\": 1.426077365875244, \"val_loss\": 1.287774682044983, \"iteration\": 21400}, {\"train_loss\": 1.418329119682312, \"val_loss\": 1.2936372756958008, \"iteration\": 21500}, {\"train_loss\": 1.4248334169387815, \"val_loss\": 1.2794476747512815, \"iteration\": 21600}, {\"train_loss\": 1.4097436666488647, \"val_loss\": 1.2679353952407837, \"iteration\": 21700}, {\"train_loss\": 1.4414159059524536, \"val_loss\": 1.2732229232788086, \"iteration\": 21800}, {\"train_loss\": 1.4192602634429932, \"val_loss\": 1.271945595741272, \"iteration\": 21900}, {\"train_loss\": 1.4168354272842407, \"val_loss\": 1.2676148414611816, \"iteration\": 22000}, {\"train_loss\": 1.4226456880569458, \"val_loss\": 1.2843273878097534, \"iteration\": 22100}, {\"train_loss\": 1.4131473302841189, \"val_loss\": 1.2642979621887207, \"iteration\": 22200}, {\"train_loss\": 1.4226144552230835, \"val_loss\": 1.2891921997070312, \"iteration\": 22300}, {\"train_loss\": 1.4182430505752563, \"val_loss\": 1.2808936834335327, \"iteration\": 22400}, {\"train_loss\": 1.4243899583816528, \"val_loss\": 1.2724465131759644, \"iteration\": 22500}, {\"train_loss\": 1.431746244430542, \"val_loss\": 1.253227949142456, \"iteration\": 22600}, {\"train_loss\": 1.4177944660186768, \"val_loss\": 1.2740790843963623, \"iteration\": 22700}, {\"train_loss\": 1.4364526271820068, \"val_loss\": 1.273636817932129, \"iteration\": 22800}, {\"train_loss\": 1.4173372983932495, \"val_loss\": 1.2570430040359497, \"iteration\": 22900}, {\"train_loss\": 1.4255906343460083, \"val_loss\": 1.2807612419128418, \"iteration\": 23000}, {\"train_loss\": 1.4129958152770996, \"val_loss\": 1.2572840452194214, \"iteration\": 23100}, {\"train_loss\": 1.4055801630020142, \"val_loss\": 1.2693111896514893, \"iteration\": 23200}, {\"train_loss\": 1.4252405166625977, \"val_loss\": 1.2652760744094849, \"iteration\": 23300}, {\"train_loss\": 1.4133384227752686, \"val_loss\": 1.2706173658370972, \"iteration\": 23400}, {\"train_loss\": 1.422166347503662, \"val_loss\": 1.2582542896270752, \"iteration\": 23500}, {\"train_loss\": 1.4206081628799438, \"val_loss\": 1.2660503387451172, \"iteration\": 23600}, {\"train_loss\": 1.4155006408691406, \"val_loss\": 1.276177167892456, \"iteration\": 23700}, {\"train_loss\": 1.4251445531845093, \"val_loss\": 1.2784972190856934, \"iteration\": 23800}, {\"train_loss\": 1.4084885120391846, \"val_loss\": 1.250560283660889, \"iteration\": 23900}, {\"train_loss\": 1.424283742904663, \"val_loss\": 1.2664906978607178, \"iteration\": 24000}, {\"train_loss\": 1.422313213348389, \"val_loss\": 1.2420949935913086, \"iteration\": 24100}, {\"train_loss\": 1.4011411666870115, \"val_loss\": 1.2685354948043823, \"iteration\": 24200}, {\"train_loss\": 1.435396432876587, \"val_loss\": 1.257452130317688, \"iteration\": 24300}, {\"train_loss\": 1.401984691619873, \"val_loss\": 1.254624605178833, \"iteration\": 24400}, {\"train_loss\": 1.399584174156189, \"val_loss\": 1.2548394203186035, \"iteration\": 24500}, {\"train_loss\": 1.4213944673538208, \"val_loss\": 1.257799506187439, \"iteration\": 24600}, {\"train_loss\": 1.4111871719360352, \"val_loss\": 1.2536274194717407, \"iteration\": 24700}, {\"train_loss\": 1.4150359630584717, \"val_loss\": 1.2528901100158691, \"iteration\": 24800}, {\"train_loss\": 1.4079370498657229, \"val_loss\": 1.256638526916504, \"iteration\": 24900}, {\"train_loss\": 1.414914846420288, \"val_loss\": 1.2446502447128296, \"iteration\": 25000}, {\"train_loss\": 1.4163419008255005, \"val_loss\": 1.2368650436401367, \"iteration\": 25100}, {\"train_loss\": 1.4018672704696655, \"val_loss\": 1.2418445348739624, \"iteration\": 25200}, {\"train_loss\": 1.43831467628479, \"val_loss\": 1.2565028667449951, \"iteration\": 25300}, {\"train_loss\": 1.3976659774780271, \"val_loss\": 1.240321397781372, \"iteration\": 25400}, {\"train_loss\": 1.4053354263305664, \"val_loss\": 1.2209831476211548, \"iteration\": 25500}, {\"train_loss\": 1.4134538173675537, \"val_loss\": 1.244083285331726, \"iteration\": 25600}, {\"train_loss\": 1.4301533699035645, \"val_loss\": 1.2504905462265017, \"iteration\": 25700}, {\"train_loss\": 1.420374870300293, \"val_loss\": 1.2518093585968018, \"iteration\": 25800}, {\"train_loss\": 1.4107022285461426, \"val_loss\": 1.226225733757019, \"iteration\": 25900}, {\"train_loss\": 1.4299546480178833, \"val_loss\": 1.2237272262573242, \"iteration\": 26000}, {\"train_loss\": 1.398604393005371, \"val_loss\": 1.2376757860183716, \"iteration\": 26100}, {\"train_loss\": 1.4167065620422363, \"val_loss\": 1.2444521188735962, \"iteration\": 26200}, {\"train_loss\": 1.3918282985687256, \"val_loss\": 1.2283531427383425, \"iteration\": 26300}, {\"train_loss\": 1.416157841682434, \"val_loss\": 1.2309974431991575, \"iteration\": 26400}, {\"train_loss\": 1.4061758518218994, \"val_loss\": 1.2506505250930786, \"iteration\": 26500}, {\"train_loss\": 1.4219647645950315, \"val_loss\": 1.2342569828033447, \"iteration\": 26600}, {\"train_loss\": 1.430089235305786, \"val_loss\": 1.2337864637374878, \"iteration\": 26700}, {\"train_loss\": 1.4077240228652954, \"val_loss\": 1.2331135272979736, \"iteration\": 26800}, {\"train_loss\": 1.419089674949646, \"val_loss\": 1.2387003898620603, \"iteration\": 26900}, {\"train_loss\": 1.404589056968689, \"val_loss\": 1.2311538457870483, \"iteration\": 27000}, {\"train_loss\": 1.39902925491333, \"val_loss\": 1.2283902168273926, \"iteration\": 27100}, {\"train_loss\": 1.4118725061416626, \"val_loss\": 1.2458926439285278, \"iteration\": 27200}, {\"train_loss\": 1.416619896888733, \"val_loss\": 1.2305335998535156, \"iteration\": 27300}, {\"train_loss\": 1.4038116931915283, \"val_loss\": 1.2263864278793335, \"iteration\": 27400}, {\"train_loss\": 1.4005141258239746, \"val_loss\": 1.218712329864502, \"iteration\": 27500}, {\"train_loss\": 1.3901828527450562, \"val_loss\": 1.2454735040664673, \"iteration\": 27600}, {\"train_loss\": 1.4231557846069336, \"val_loss\": 1.2158687114715576, \"iteration\": 27700}, {\"train_loss\": 1.3944131135940552, \"val_loss\": 1.2429184913635254, \"iteration\": 27800}, {\"train_loss\": 1.4105863571166992, \"val_loss\": 1.2307255268096924, \"iteration\": 27900}, {\"train_loss\": 1.4003030061721802, \"val_loss\": 1.2175755500793457, \"iteration\": 28000}, {\"train_loss\": 1.3924925327301023, \"val_loss\": 1.2305638790130615, \"iteration\": 28100}, {\"train_loss\": 1.4142810106277466, \"val_loss\": 1.2155722379684448, \"iteration\": 28200}, {\"train_loss\": 1.4023218154907229, \"val_loss\": 1.232231616973877, \"iteration\": 28300}, {\"train_loss\": 1.407426118850708, \"val_loss\": 1.217682957649231, \"iteration\": 28400}, {\"train_loss\": 1.4110496044158936, \"val_loss\": 1.2219820022583008, \"iteration\": 28500}, {\"train_loss\": 1.410663604736328, \"val_loss\": 1.2150428295135498, \"iteration\": 28600}, {\"train_loss\": 1.4054816961288452, \"val_loss\": 1.207146167755127, \"iteration\": 28700}, {\"train_loss\": 1.389340043067932, \"val_loss\": 1.2265496253967283, \"iteration\": 28800}, {\"train_loss\": 1.3965033292770386, \"val_loss\": 1.2168173789978027, \"iteration\": 28900}, {\"train_loss\": 1.4174249172210691, \"val_loss\": 1.1975170373916626, \"iteration\": 29000}, {\"train_loss\": 1.39912748336792, \"val_loss\": 1.2086743116378784, \"iteration\": 29100}, {\"train_loss\": 1.3955518007278442, \"val_loss\": 1.220542550086975, \"iteration\": 29200}, {\"train_loss\": 1.4001243114471436, \"val_loss\": 1.2074650526046753, \"iteration\": 29300}, {\"train_loss\": 1.394925832748413, \"val_loss\": 1.213141679763794, \"iteration\": 29400}, {\"train_loss\": 1.411750078201294, \"val_loss\": 1.2162675857543943, \"iteration\": 29500}, {\"train_loss\": 1.406941533088684, \"val_loss\": 1.2150543928146362, \"iteration\": 29600}, {\"train_loss\": 1.3924604654312134, \"val_loss\": 1.2147740125656128, \"iteration\": 29700}, {\"train_loss\": 1.4004714488983154, \"val_loss\": 1.2093195915222168, \"iteration\": 29800}, {\"train_loss\": 1.4023529291152954, \"val_loss\": 1.2192548513412476, \"iteration\": 29900}, {\"train_loss\": 1.4094597101211548, \"val_loss\": 1.1969074010849, \"iteration\": 30000}, {\"train_loss\": 1.4096015691757202, \"val_loss\": 1.19411039352417, \"iteration\": 30100}, {\"train_loss\": 1.40427827835083, \"val_loss\": 1.220121145248413, \"iteration\": 30200}, {\"train_loss\": 1.3835052251815796, \"val_loss\": 1.2097651958465576, \"iteration\": 30300}, {\"train_loss\": 1.384865164756775, \"val_loss\": 1.2113142013549805, \"iteration\": 30400}, {\"train_loss\": 1.3965089321136477, \"val_loss\": 1.2195221185684204, \"iteration\": 30500}, {\"train_loss\": 1.3851245641708374, \"val_loss\": 1.2029567956924438, \"iteration\": 30600}, {\"train_loss\": 1.3880565166473389, \"val_loss\": 1.217883586883545, \"iteration\": 30700}, {\"train_loss\": 1.4056315422058103, \"val_loss\": 1.229143500328064, \"iteration\": 30800}, {\"train_loss\": 1.413903832435608, \"val_loss\": 1.2059508562088013, \"iteration\": 30900}, {\"train_loss\": 1.3760837316513062, \"val_loss\": 1.1972322463989258, \"iteration\": 31000}, {\"train_loss\": 1.3799636363983154, \"val_loss\": 1.2140008211135864, \"iteration\": 31100}, {\"train_loss\": 1.389432072639465, \"val_loss\": 1.2060823440551758, \"iteration\": 31200}, {\"train_loss\": 1.400603175163269, \"val_loss\": 1.2001761198043823, \"iteration\": 31300}, {\"train_loss\": 1.407656192779541, \"val_loss\": 1.18342387676239, \"iteration\": 31400}, {\"train_loss\": 1.387820839881897, \"val_loss\": 1.1967560052871704, \"iteration\": 31500}, {\"train_loss\": 1.3972378969192505, \"val_loss\": 1.2034292221069336, \"iteration\": 31600}, {\"train_loss\": 1.4012683629989624, \"val_loss\": 1.1921018362045288, \"iteration\": 31700}, {\"train_loss\": 1.4050588607788086, \"val_loss\": 1.1988365650177002, \"iteration\": 31800}, {\"train_loss\": 1.3842676877975464, \"val_loss\": 1.1960638761520386, \"iteration\": 31900}, {\"train_loss\": 1.414679765701294, \"val_loss\": 1.201311707496643, \"iteration\": 32000}, {\"train_loss\": 1.375817894935608, \"val_loss\": 1.206432580947876, \"iteration\": 32100}, {\"train_loss\": 1.4044923782348633, \"val_loss\": 1.1997878551483154, \"iteration\": 32200}, {\"train_loss\": 1.3889557123184204, \"val_loss\": 1.1878924369812012, \"iteration\": 32300}, {\"train_loss\": 1.3906280994415283, \"val_loss\": 1.182062745094299, \"iteration\": 32400}, {\"train_loss\": 1.380386233329773, \"val_loss\": 1.1806219816207886, \"iteration\": 32500}, {\"train_loss\": 1.392003297805786, \"val_loss\": 1.1927025318145752, \"iteration\": 32600}, {\"train_loss\": 1.410580039024353, \"val_loss\": 1.1791445016860962, \"iteration\": 32700}, {\"train_loss\": 1.404862403869629, \"val_loss\": 1.1967889070510864, \"iteration\": 32800}, {\"train_loss\": 1.3894774913787842, \"val_loss\": 1.190437078475952, \"iteration\": 32900}, {\"train_loss\": 1.4038807153701782, \"val_loss\": 1.1898996829986572, \"iteration\": 33000}, {\"train_loss\": 1.3982402086257937, \"val_loss\": 1.183834195137024, \"iteration\": 33100}, {\"train_loss\": 1.3890092372894287, \"val_loss\": 1.1906371116638184, \"iteration\": 33200}, {\"train_loss\": 1.3834224939346311, \"val_loss\": 1.1878572702407837, \"iteration\": 33300}, {\"train_loss\": 1.3906766176223757, \"val_loss\": 1.1875286102294922, \"iteration\": 33400}, {\"train_loss\": 1.4023518562316897, \"val_loss\": 1.1795835494995115, \"iteration\": 33500}, {\"train_loss\": 1.4130252599716189, \"val_loss\": 1.1909985542297363, \"iteration\": 33600}, {\"train_loss\": 1.3951572179794312, \"val_loss\": 1.182390213012695, \"iteration\": 33700}, {\"train_loss\": 1.3807066679000854, \"val_loss\": 1.1812000274658203, \"iteration\": 33800}, {\"train_loss\": 1.4070903062820437, \"val_loss\": 1.178699016571045, \"iteration\": 33900}, {\"train_loss\": 1.3882741928100586, \"val_loss\": 1.1838631629943848, \"iteration\": 34000}, {\"train_loss\": 1.3934030532836914, \"val_loss\": 1.193085551261902, \"iteration\": 34100}, {\"train_loss\": 1.390622615814209, \"val_loss\": 1.1827683448791504, \"iteration\": 34200}, {\"train_loss\": 1.3911079168319702, \"val_loss\": 1.1919764280319214, \"iteration\": 34300}, {\"train_loss\": 1.3961586952209473, \"val_loss\": 1.1857759952545166, \"iteration\": 34400}, {\"train_loss\": 1.3839994668960571, \"val_loss\": 1.1826733350753784, \"iteration\": 34500}, {\"train_loss\": 1.390416979789734, \"val_loss\": 1.1735814809799194, \"iteration\": 34600}, {\"train_loss\": 1.386060357093811, \"val_loss\": 1.168606162071228, \"iteration\": 34700}, {\"train_loss\": 1.3860046863555908, \"val_loss\": 1.1753108501434326, \"iteration\": 34800}, {\"train_loss\": 1.4054255485534668, \"val_loss\": 1.1812677383422852, \"iteration\": 34900}, {\"train_loss\": 1.3814327716827393, \"val_loss\": 1.1851550340652466, \"iteration\": 35000}, {\"train_loss\": 1.3938591480255127, \"val_loss\": 1.193217158317566, \"iteration\": 35100}, {\"train_loss\": 1.388116717338562, \"val_loss\": 1.1797478199005127, \"iteration\": 35200}, {\"train_loss\": 1.3867175579071045, \"val_loss\": 1.187743902206421, \"iteration\": 35300}, {\"train_loss\": 1.3829628229141235, \"val_loss\": 1.169634461402893, \"iteration\": 35400}, {\"train_loss\": 1.407559514045715, \"val_loss\": 1.16476571559906, \"iteration\": 35500}, {\"train_loss\": 1.3776990175247192, \"val_loss\": 1.154279351234436, \"iteration\": 35600}, {\"train_loss\": 1.3889648914337158, \"val_loss\": 1.1749663352966309, \"iteration\": 35700}, {\"train_loss\": 1.399950385093689, \"val_loss\": 1.1907281875610352, \"iteration\": 35800}, {\"train_loss\": 1.3752036094665527, \"val_loss\": 1.185216784477234, \"iteration\": 35900}, {\"train_loss\": 1.392662525177002, \"val_loss\": 1.1605408191680908, \"iteration\": 36000}, {\"train_loss\": 1.377273678779602, \"val_loss\": 1.171614170074463, \"iteration\": 36100}, {\"train_loss\": 1.3851655721664429, \"val_loss\": 1.1646153926849363, \"iteration\": 36200}, {\"train_loss\": 1.379520058631897, \"val_loss\": 1.1664915084838867, \"iteration\": 36300}, {\"train_loss\": 1.397944688796997, \"val_loss\": 1.1706546545028689, \"iteration\": 36400}, {\"train_loss\": 1.3780272006988523, \"val_loss\": 1.1632405519485474, \"iteration\": 36500}, {\"train_loss\": 1.3697465658187866, \"val_loss\": 1.1670844554901123, \"iteration\": 36600}, {\"train_loss\": 1.388837456703186, \"val_loss\": 1.1655967235565186, \"iteration\": 36700}, {\"train_loss\": 1.3850420713424685, \"val_loss\": 1.1601932048797607, \"iteration\": 36800}, {\"train_loss\": 1.392305612564087, \"val_loss\": 1.1694817543029783, \"iteration\": 36900}, {\"train_loss\": 1.3827368021011353, \"val_loss\": 1.1612880229949951, \"iteration\": 37000}, {\"train_loss\": 1.3815492391586304, \"val_loss\": 1.148825764656067, \"iteration\": 37100}, {\"train_loss\": 1.3998249769210815, \"val_loss\": 1.1662521362304688, \"iteration\": 37200}, {\"train_loss\": 1.386027216911316, \"val_loss\": 1.1564512252807615, \"iteration\": 37300}, {\"train_loss\": 1.4007289409637451, \"val_loss\": 1.1624523401260376, \"iteration\": 37400}, {\"train_loss\": 1.391108512878418, \"val_loss\": 1.1605076789855957, \"iteration\": 37500}, {\"train_loss\": 1.3767181634902954, \"val_loss\": 1.1523164510726929, \"iteration\": 37600}, {\"train_loss\": 1.3868811130523682, \"val_loss\": 1.1558059453964231, \"iteration\": 37700}, {\"train_loss\": 1.3985681533813477, \"val_loss\": 1.157941818237305, \"iteration\": 37800}, {\"train_loss\": 1.3896915912628174, \"val_loss\": 1.162616491317749, \"iteration\": 37900}, {\"train_loss\": 1.400424242019653, \"val_loss\": 1.1628389358520508, \"iteration\": 38000}, {\"train_loss\": 1.3905649185180664, \"val_loss\": 1.1518301963806152, \"iteration\": 38100}, {\"train_loss\": 1.4021846055984497, \"val_loss\": 1.1607365608215332, \"iteration\": 38200}, {\"train_loss\": 1.394850254058838, \"val_loss\": 1.1606581211090088, \"iteration\": 38300}, {\"train_loss\": 1.397982478141785, \"val_loss\": 1.1585386991500854, \"iteration\": 38400}, {\"train_loss\": 1.387681245803833, \"val_loss\": 1.1515851020812988, \"iteration\": 38500}, {\"train_loss\": 1.3766305446624756, \"val_loss\": 1.140957236289978, \"iteration\": 38600}, {\"train_loss\": 1.3903847932815552, \"val_loss\": 1.159238576889038, \"iteration\": 38700}, {\"train_loss\": 1.403700828552246, \"val_loss\": 1.15053391456604, \"iteration\": 38800}, {\"train_loss\": 1.3736706972122192, \"val_loss\": 1.173988938331604, \"iteration\": 38900}, {\"train_loss\": 1.4035251140594482, \"val_loss\": 1.1480627059936523, \"iteration\": 39000}, {\"train_loss\": 1.3855433464050293, \"val_loss\": 1.161709189414978, \"iteration\": 39100}, {\"train_loss\": 1.3800636529922483, \"val_loss\": 1.1499245166778564, \"iteration\": 39200}, {\"train_loss\": 1.3944331407546997, \"val_loss\": 1.1546589136123655, \"iteration\": 39300}, {\"train_loss\": 1.4020888805389404, \"val_loss\": 1.1545759439468384, \"iteration\": 39400}, {\"train_loss\": 1.3927992582321167, \"val_loss\": 1.1410470008850098, \"iteration\": 39500}, {\"train_loss\": 1.3779296875, \"val_loss\": 1.1541639566421509, \"iteration\": 39600}, {\"train_loss\": 1.402488350868225, \"val_loss\": 1.1580307483673096, \"iteration\": 39700}, {\"train_loss\": 1.3883646726608276, \"val_loss\": 1.1516175270080566, \"iteration\": 39800}, {\"train_loss\": 1.3790897130966189, \"val_loss\": 1.1563317775726318, \"iteration\": 39900}, {\"train_loss\": 1.3951094150543213, \"val_loss\": 1.159595012664795, \"iteration\": 40000}, {\"train_loss\": 1.364589810371399, \"val_loss\": 1.1334019899368286, \"iteration\": 40100}, {\"train_loss\": 1.3934098482131958, \"val_loss\": 1.145246505737305, \"iteration\": 40200}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.LayerChart(...)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alt.Chart(d).mark_line().encode(\n",
    "    x=alt.X(\"iteration\"),\n",
    "    y=alt.Y(\"val_loss\"),\n",
    ") + alt.Chart(d).mark_line().encode(\n",
    "    x=alt.X(\"iteration\"),\n",
    "    y=alt.Y(\"train_loss\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
